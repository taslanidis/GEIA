#!/bin/bash
#SBATCH --partition=gpu_a100
#SBATCH --gpus=1
#SBATCH --gpus-per-node=1
#SBATCH --job-name=detect_leakage
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=18
#SBATCH --time=00:59:00
#SBATCH --output=./output/%x-%j.out

# entire script fails if a single command fails
set -e

module purge
module load 2023
module load Anaconda3/2023.07-2

PROJECT_DIR="$PWD"
ENV_PREFIX="$PROJECT_DIR"/.env_extension

source activate $ENV_PREFIX

python -c "import torch; print('CUDA available:', torch.cuda.is_available()); print('CUDA device count:', torch.cuda.device_count()); print('CUDA device name:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No CUDA device')"

# victim: sent roberta
python detect_training_leakage.py --dataset_path LLM_instruct_masking/output_10k_rows.jsonl

python detect_training_leakage.py --dataset_path LLM_instruct_masking/output_10k_rows.jsonl --exclude_sentence_emb

python detect_training_leakage.py --dataset_path LLM_instruct_masking/llama_masking-128_80_iterations.jsonl

python detect_training_leakage.py --dataset_path LLM_instruct_masking/llama_masking-128_80_iterations.jsonl --exclude_sentence_emb

# victim: simcse bert
python detect_training_leakage.py --embed_model simcse_bert --dataset_path LLM_instruct_masking/output_10k_rows.jsonl

python detect_training_leakage.py --embed_model simcse_bert --dataset_path LLM_instruct_masking/output_10k_rows.jsonl --exclude_sentence_emb

python detect_training_leakage.py --embed_model simcse_bert --dataset_path LLM_instruct_masking/llama_masking-128_80_iterations.jsonl

python detect_training_leakage.py --embed_model simcse_bert --dataset_path LLM_instruct_masking/llama_masking-128_80_iterations.jsonl --exclude_sentence_emb

